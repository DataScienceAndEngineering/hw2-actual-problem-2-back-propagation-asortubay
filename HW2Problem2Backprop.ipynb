{"cells":[{"cell_type":"markdown","metadata":{"id":"xuuA_SHMuUne"},"source":["# HW2: Problem 2: Working out Backpropagation\n","\n","Read Chapter 2 of Michael Nielsen's article/book from top to bottom:\n","\n","* [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)\n","\n","He outlines a few exersizes in that article which you must complete. Do the following a, b, c:\n","\n","a. He invites you to write out a proof of equation BP3\n","\n","b. He invites you to write out a proof of equation BP4\n","\n","c. He proposes that you code a fully matrix-based approach to backpropagation over a mini-batch. Implement this with explanation where you change the notation so that instead of having a bias term, you assume that the input variables are augmented with a \"column\" of \"1\"s, and that the weights $w_0$.\n","\n","Your submission should be a single jupyter notebook. Use markdown cells with latex for equations of a jupyter notebook for each proof for \"a.\" and \"b.\". Make sure you include text that explains your steps. Next for \"c\" this is an implementation problem. You need to understand and modify the code the Michael Nielsen provided so that instead it is using a matrixed based approach. Again don't keep the biases separate. After reading data in (use the iris data set), create a new column corresponding to $x_0=1$, and as mentioned above and discussed in class (see notes) is that the bias term can then be considered a weight $w_0$. Again use markdown cells around your code and comments to explain your work. Test the code on the iris data set with 4 node input (5 with a constant 1), three hidden nodes, and three output nodes, one for each species/class."]},{"cell_type":"markdown","metadata":{"id":"NsfIQN1-uwGB"},"source":["## a. Proof of Michael Nielsons equation BP3\n","Equation 3:\n","\n","$\\frac{\\partial C}{\\partial b_j^l}$ = $\\delta_j^l$\n","\n","\n","Then we can apply the chain rule to find the derivative of C with respect to b.\n","\n","\n","$\\frac{\\partial C}{\\partial b_j^l}$ = $\\frac{\\partial C}{\\partial Z_j^l} . \\frac{\\partial Z_j^l}{\\partial b_j^l}$\n","\n","If we know that\n","\n","$\\frac{\\partial C}{\\partial Z_j^l}$ = $\\delta_j^l$\n","\n","then we get:\n","\n","$\\frac{\\partial C}{\\partial b_j^l}$ =  $\\delta_j^l$ . $\\frac{\\partial Z_j^l}{\\partial b_j^l}$\n","\n","\n","We know that b is a linear transformation of Z:\n","\n","$Z_j^l = \\sum_k w_{jk}^l a_k^{l-1} + b_j^l$\n","\n","Then we can find the derivative of Z with respect to b, which is simply 1:\n","\n","$\\frac{\\partial Z_j^l}{\\partial b_j^l}$ = 1\n","\n","Then we finally arrive at the initial BP3 equation:\n","\n","$\\frac{\\partial C}{\\partial b_j^l}$ = $\\delta_j^l$"]},{"cell_type":"markdown","metadata":{"id":"axsx4eu9u_3x"},"source":["## b. Proof of Michael Nielsons equation BP4\n","\n","Equation 4:\n","\n","\n","$\\frac{\\partial C}{\\partial w_{jk}^l}$ = $\\delta_j^l$ . $a_k^{l-1}$\n","\n","We can again apply the chain rule:\n","\n","$\\frac{\\partial C}{\\partial w_{jk}^l}$ = $\\frac{\\partial C}{\\partial Z_j^l} . \\frac{\\partial Z_j^l}{\\partial w_{jk}^l}$\n","\n","Because z is a linear transformation of w:\n","\n","$Z_j^l = \\sum_k w_{jk}^l a_k^{l-1} + b_j^l$\n","\n","we can find the derivative of Z with respect to w:\n","\n","$\\frac{\\partial Z_j^l}{\\partial w_{jk}^l}$ = $a_k^{l-1}$\n","\n","and as we also know that: \n","\n","$\\frac{\\partial C}{\\partial Z_j^l}$ = $\\delta_j^l$\n","\n","And we finally arrive at the initial BP4 equation:\n","\n","$\\frac{\\partial C}{\\partial w_{jk}^l}$ = $\\delta_j^l$ . $a_k^{l-1}$\n"]},{"cell_type":"markdown","metadata":{"id":"rj1HbN9RvVXe"},"source":["## c. Using both markdown cells and code cells implement that you code a fully matrix-based approach to backpropagation over a mini-batch. Implement this with explanation where you change the notation so that instead of having a bias term, you assume that the input variables are augmented with a \"column\" of \"1\"s, and that the weights $w_0$."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1676928890900,"user":{"displayName":"Michael Grossberg","userId":"10616474120098361836"},"user_tz":300},"id":"3wUek4Nau7x7"},"outputs":[],"source":["\n","import random\n","import numpy as np\n","import pandas as pd\n","\n","class Network(object):\n","\n","    def __init__(self, sizes):\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x)\n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","\n","    def feedforward(self, a):\n","        for b, w in zip(self.biases, self.weights):\n","            a = sigmoid(np.dot(w, a) + b)\n","        return a\n","\n","    def SGD(self, training_data, epochs, mini_batch_size, eta,\n","            test_data=None):\n","        training_data = list(training_data)\n","        n = len(training_data)\n","\n","        if test_data:\n","            test_data = list(test_data)\n","            n_test = len(test_data)\n","\n","        for j in range(epochs):\n","            random.shuffle(training_data)\n","            mini_batches = [\n","                training_data[k:k + mini_batch_size]\n","                for k in range(0, n, mini_batch_size)]\n","            for mini_batch in mini_batches:\n","                self.update_mini_batch(mini_batch, eta)\n","            if test_data:\n","                print(\"Epoch {} : {} / {}\".format(j, self.evaluate(test_data), n_test))\n","            else:\n","                print(\"Epoch {} complete\".format(j))\n","\n","    def update_mini_batch(self, mini_batch, eta):\n","        nabla_b = [np.zeros(b.shape) for b in self.biases]\n","        nabla_w = [np.zeros(w.shape) for w in self.weights]\n","\n","        x_matrix_0 = [x for x, y in mini_batch]\n","        y_matrix_0 = [y for x, y in mini_batch]\n","        x_matrix = np.concatenate(x_matrix_0, axis=1)\n","        y_matrix = np.concatenate(y_matrix_0, axis=1)\n","\n","        nabla_b, nabla_w = self.backprop(x_matrix, y_matrix)\n","\n","        self.weights = [w - (eta / len(mini_batch)) * nw\n","                        for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b - (eta / len(mini_batch)) * nb\n","                    for b, nb in zip(self.biases, nabla_b)]\n","\n","    def backprop(self, x, y):\n","        nabla_b = [np.zeros(b.shape) for b in self.biases]\n","        nabla_w = [np.zeros(w.shape) for w in self.weights]\n","        # feedforward\n","        activation = x\n","        activations = [x]  # list to store all the activations, layer by layer\n","        zs = []  # list to store all the z vectors, layer by layer\n","        for b, w in zip(self.biases, self.weights):\n","            z = np.dot(w, activation) + np.kron(b, np.ones([1, y.shape[1]]))\n","            zs.append(z)\n","            activation = sigmoid(z)\n","            activations.append(activation)\n","        # backward pass\n","        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n","        nabla_b[-1] = np.reshape([np.sum(nb) for nb in delta], [delta.shape[0], 1])\n","        for _d, _a in zip(delta.transpose(), activations[-2].transpose()):\n","            _d = np.reshape(_d, [len(_d), 1])\n","            _a = np.reshape(_a, [len(_a), 1])\n","            nabla_w[-1] += np.dot(_d, _a.transpose())\n","\n","        for l in range(2, self.num_layers):\n","            z = zs[-l]\n","            sp = sigmoid_prime(z)\n","            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n","            nabla_b[-l] = np.reshape([np.sum(nb) for nb in delta], [delta.shape[0], 1])\n","            for _d, _a in zip(delta.transpose(), activations[-l-1].transpose()):\n","                _d = np.reshape(_d, [len(_d), 1])\n","                _a = np.reshape(_a, [len(_a), 1])\n","                nabla_w[-l] += np.dot(_d, _a.transpose())\n","        return nabla_b, nabla_w\n","\n","    def cost_derivative(self, output_activations, y):\n","        return (output_activations - y)\n","\n","    def sigmoid(z):\n","        return 1.0 / (1.0 + np.exp(-z))\n","\n","\n","    def sigmoid_prime(z):\n","        return sigmoid(z) * (1 - sigmoid(z))\n","\n","    "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#### Miscellaneous functions\n","def sigmoid(z):\n","    \"\"\"The sigmoid function.\"\"\"\n","    return 1.0/(1.0+np.exp(-z))\n","\n","def sigmoid_prime(z):\n","    \"\"\"Derivative of the sigmoid function.\"\"\"\n","    return sigmoid(z)*(1-sigmoid(z))\n","class Network(object):\n","\n","    def __init__(self, sizes):\n","        \"\"\"The list ``sizes`` contains the number of neurons in the\n","        respective layers of the network.  For example, if the list\n","        was [2, 3, 1] then it would be a three-layer network, with the\n","        first layer containing 2 neurons, the second layer 3 neurons,\n","        and the third layer 1 neuron.  The biases and weights for the\n","        network are initialized randomly, using a Gaussian\n","        distribution with mean 0, and variance 1.  Note that the first\n","        layer is assumed to be an input layer, and by convention we\n","        won't set any biases for those neurons, since biases are only\n","        ever used in computing the outputs from later layers.\"\"\"\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x)\n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","\n","    def feedforward(self, a):\n","        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n","        for b, w in zip(self.biases, self.weights):\n","            a = sigmoid(np.dot(w, a)+b)\n","        return a\n","\n","    def SGD(self, training_data, epochs, mini_batch_size, eta,\n","            test_data=None):\n","        \"\"\"Train the neural network using mini-batch stochastic\n","        gradient descent.  The ``training_data`` is a list of tuples\n","        ``(x, y)`` representing the training inputs and the desired\n","        outputs.  The other non-optional parameters are\n","        self-explanatory.  If ``test_data`` is provided then the\n","        network will be evaluated against the test data after each\n","        epoch, and partial progress printed out.  This is useful for\n","        tracking progress, but slows things down substantially.\"\"\"\n","        if test_data: n_test = len(test_data)\n","        n = len(training_data)\n","        for j in range(epochs):\n","            random.shuffle(training_data)\n","            mini_batches = [\n","                training_data[k:k+mini_batch_size]\n","                for k in range(0, n, mini_batch_size)]\n","            for mini_batch in mini_batches:\n","                self.update_mini_batch(mini_batch, eta)\n","            if test_data:\n","                print(\"Epoch {0}: {1} / {2}\".format(\n","                    j, self.evaluate(test_data), n_test))\n","            else:\n","                print(\"Epoch {0} complete\".format(j))\n","\n","    def update_mini_batch(self, mini_batch, eta):\n","        \"\"\"Update the network's weights and biases by applying\n","        gradient descent using backpropagation to a single mini batch.\n","        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n","        is the learning rate.\"\"\"\n","        nabla_b = [np.zeros(b.shape) for b in self.biases]\n","        nabla_w = [np.zeros(w.shape) for w in self.weights]\n","        for x, y in mini_batch:\n","            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n","            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n","            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n","        self.weights = [w-(eta/len(mini_batch))*nw\n","                        for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b-(eta/len(mini_batch))*nb\n","                       for b, nb in zip(self.biases, nabla_b)]\n","\n","    def backprop(self, x, y):\n","        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n","        gradient for the cost function C_x.  ``nabla_b`` and\n","        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n","        to ``self.biases`` and ``self.weights``.\"\"\"\n","        nabla_b = [np.zeros(b.shape) for b in self.biases]\n","        nabla_w = [np.zeros(w.shape) for w in self.weights]\n","        # feedforward\n","        activation = x\n","        activations = [x] # list to store all the activations, layer by layer\n","        zs = [] # list to store all the z vectors, layer by layer\n","        for b, w in zip(self.biases, self.weights):\n","            z = np.dot(w, activation)+b\n","            zs.append(z)\n","            activation = sigmoid(z)\n","            activations.append(activation)\n","        # backward pass\n","        delta = self.cost_derivative(activations[-1], y) * \\\n","            sigmoid_prime(zs[-1])\n","        nabla_b[-1] = delta\n","        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n","        # Note that the variable l in the loop below is used a little\n","        # differently to the notation in Chapter 2 of the book.  Here,\n","        # l = 1 means the last layer of neurons, l = 2 is the\n","        # second-last layer, and so on.  It's a renumbering of the\n","        # scheme in the book, used here to take advantage of the fact\n","        # that Python can use negative indices in lists.\n","        for l in range(2, self.num_layers):\n","            z = zs[-l]\n","            sp = sigmoid_prime(z)\n","            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n","            nabla_b[-l] = delta\n","            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n","        return (nabla_b, nabla_w)\n","\n","    def evaluate(self, test_data):\n","        \"\"\"Return the number of test inputs for which the neural\n","        network outputs the correct result. Note that the neural\n","        network's output is assumed to be the index of whichever\n","        neuron in the final layer has the highest activation.\"\"\"\n","        test_results = [(np.argmax(self.feedforward(x)), y)\n","                        for (x, y) in test_data]\n","        return sum(int(x == y) for (x, y) in test_results)\n","\n","    def cost_derivative(self, output_activations, y):\n","        \"\"\"Return the vector of partial derivatives \\partial C_x /\n","        \\partial a for the output activations.\"\"\"\n","        return (output_activations-y)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","\n","# Load the iris dataset\n","iris = load_iris()\n","\n","# Shuffle the dataset\n","indices = np.arange(iris.data.shape[0])\n","np.random.shuffle(indices)\n","\n","# Split the dataset into a training set and a test set\n","train_size = int(0.8 * len(indices))  # 80% of data for training\n","train_indices = indices[:train_size]\n","test_indices = indices[train_size:]\n","\n","def one_hot_encode(y, num_classes):\n","    return np.eye(num_classes)[y] \n","X =iris.data\n","y = iris.target\n","#####\n","data = pd.DataFrame(X, columns=iris.feature_names)\n","data.insert(0, \"bias padding\", 1,)\n","\n","\n","data['label'] = y\n","\n","# X = np.insert(X, 0, 1, axis=1)\n","\n","X = np.insert(X, 0, 1, axis=1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","def vectorized_result(j,nClasses):\n","    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n","    position and zeroes elsewhere.  This is used to convert a class\n","    (0...n) into a corresponding desired output from the neural\n","    network.\"\"\"\n","    e = np.zeros((nClasses, 1))\n","    e[j] = 1.0\n","    return e\n","\n","training_inputs = [np.reshape(x, (5, 1)) for x in X_train]\n","training_results = [vectorized_result(y,3) for y in y_train]\n","training_data = list(zip(training_inputs, training_results))\n","validation_inputs = [np.reshape(x, (5, 1)) for x in X_test]\n","validation_data = list(zip(validation_inputs, y_test))\n","\n","model = Network([5, 3, 3])\n","model.SGD(training_data=training_data,epochs = 30, mini_batch_size = 10, eta = 3, test_data=validation_data)\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPGRo7E+vvs1+9PrlkZvkBC","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
